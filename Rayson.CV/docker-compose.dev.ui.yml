# ===========================================
# Docker Compose - Development: UI
# ===========================================
# Usage: docker compose -f docker-compose.dev.ui.yml up -d
# Purpose: Run UI + Ollama in Docker, debug API locally
# API runs on host at http://localhost:13245
# ===========================================

services:
  ui:
    build:
      context: ./UI
      dockerfile: Dockerfile
      args:
        VITE_API_BASE_URL: http://localhost:13245/
    container_name: raysoncv-ui
    restart: unless-stopped
    environment:
      API_HEALTH_URL: http://host.docker.internal:13245/health
      LOG_LEVEL: info
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ===========================================
  # Ollama (AI Chatbot)
  # ===========================================
  ollama:
    build:
      context: .
      dockerfile: ollama.Dockerfile
    container_name: raysoncv-ollama
    restart: unless-stopped
    environment:
      OLLAMA_HOST: 0.0.0.0
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11435:11434"
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:11434/api/tags | grep -q tinyllama"]
      interval: 5s
      timeout: 10s
      retries: 180
      start_period: 0s

networks:
  internal:
    driver: bridge

volumes:
  ollama-data:
