# ===========================================
# Docker Compose - Development: Full Stack
# ===========================================
# Usage: docker compose -f docker-compose.dev.full.yml up -d
# Purpose: Run all services (API, UI, Ollama) in Docker
# ===========================================

services:
  # ===========================================
  # API (.NET 8.0)
  # ===========================================
  api:
    build:
      context: ./Api
      dockerfile: Dockerfile
    container_name: raysoncv-api
    restart: unless-stopped
    environment:
      ASPNETCORE_ENVIRONMENT: Development
      Cors__AllowedOrigins__0: http://localhost:3000
      LOG_LEVEL: Debug
      OLLAMA__BASEURL: http://ollama:11434
    ports:
      - "13245:8080"
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ===========================================
  # UI (React/Vite)
  # ===========================================
  ui:
    build:
      context: ./UI
      dockerfile: Dockerfile
      args:
        VITE_API_BASE_URL: ${VITE_API_BASE_URL}
    container_name: raysoncv-ui
    restart: unless-stopped
    environment:
      API_HEALTH_URL: http://api:8080/health
      LOG_LEVEL: info
    ports:
      - "3000:3000"
    depends_on:
      api:
        condition: service_healthy
    networks:
      - internal
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ===========================================
  # Ollama (AI Chatbot)
  # ===========================================
  ollama:
    build:
      context: .
      dockerfile: ollama.Dockerfile
    container_name: raysoncv-ollama
    restart: unless-stopped
    environment:
      OLLAMA_HOST: 0.0.0.0
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11435:11434"
    networks:
      - internal
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:11434/api/tags | grep -q tinyllama"]
      interval: 5s
      timeout: 10s
      retries: 180
      start_period: 0s

# ===========================================
# Networks
# ===========================================
networks:
  internal:
    driver: bridge

# ===========================================
# Volumes
# ===========================================
volumes:
  ollama-data:
